import requests
import feedparser
from datetime import datetime
from bs4 import BeautifulSoup
import re
import time
from kafka import KafkaProducer
from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy import select
import json

# âœ… DB ì—°ê²° ë° í…Œì´ë¸” ì¤€ë¹„
engine = create_engine("postgresql+psycopg2://postgres:ssafy@localhost:5432/news")
metadata = MetaData()
metadata.reflect(bind=engine)
news_article = metadata.tables['news_article']

# âœ… í—¤ë” (403 ìš°íšŒ)
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/115.0.0.0 Safari/537.36"
}

# âœ… ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
def get_korean_text_from_url(url):
    try:
        res = requests.get(url, headers=headers, timeout=5)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ìš”ì²­ ì‹¤íŒ¨: {url} | {e}")
        return ""

    soup = BeautifulSoup(res.text, "html.parser")
    text_list = []

    for string in soup.stripped_strings:
        if re.search(r"[ê°€-í£]{2,}", string) and len(string.strip()) >= 20:
            text_list.append(string.strip())

    return " ".join(text_list)

# âœ… ì‚½ì… í•¨ìˆ˜
def insert_article(title, url, writer, summary, write_date, full_text):
    with engine.begin() as conn:
        stmt = pg_insert(news_article).values(
            url=url,
            title=title,
            writer=writer,
            summary=summary,
            write_date=write_date.date() if isinstance(write_date, datetime) else write_date,
            full_text=full_text
        ).on_conflict_do_nothing(index_elements=['url'])

        conn.execute(stmt)
        print(f"âœ… ì‚½ì… ì™„ë£Œ: {title}")

# âœ… ì¤‘ë³µ ì—¬ë¶€ ê²€ì‚¬ í•¨ìˆ˜
def article_exists(conn, url):
    query = news_article.select().where(news_article.c.url == url)
    return conn.execute(query).fetchone() is not None

# âœ… RSS í”¼ë“œ URL (Khan ë‰´ìŠ¤ RSS ì˜ˆì‹œ)
RSS_FEED_URL_LIST = ["https://www.khan.co.kr/rss/rssdata/total_news.xml",       # ê²½í–¥ì‹ ë¬¸ / ì „ì²´ ë‰´ìŠ¤
                    "https://www.khan.co.kr/rss/rssdata/cartoon_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ë§Œí‰
                    "https://www.khan.co.kr/rss/rssdata/opinion_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ì˜¤í”¼ë‹ˆì–¸
                    "https://www.khan.co.kr/rss/rssdata/politic_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ì •ì¹˜
                    "https://www.khan.co.kr/rss/rssdata/economy_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ê²½ì œ
                    "https://www.khan.co.kr/rss/rssdata/society_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ì‚¬íšŒ
                    "https://www.khan.co.kr/rss/rssdata/local_news.xml",        # ê²½í–¥ì‹ ë¬¸ / ì§€ì—­
                    "https://www.khan.co.kr/rss/rssdata/kh_world.xml",          # ê²½í–¥ì‹ ë¬¸ / êµ­ì œ
                    "https://www.khan.co.kr/rss/rssdata/culture_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ë¬¸í™”
                    "https://www.khan.co.kr/rss/rssdata/kh_sports.xml",         # ê²½í–¥ì‹ ë¬¸ / ìŠ¤í¬ì¸ 
                    "https://www.khan.co.kr/rss/rssdata/science_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ê³¼í•™&í™˜ê²½
                    "https://www.khan.co.kr/rss/rssdata/life_news.xml",         # ê²½í–¥ì‹ ë¬¸ / ë¼ì´í”„
                    "https://www.khan.co.kr/rss/rssdata/people_news.xml",       # ê²½í–¥ì‹ ë¬¸ / ì‚¬ëŒ
                    "https://www.khan.co.kr/rss/rssdata/english_news.xml",      # ê²½í–¥ì‹ ë¬¸ / ì˜ë¬¸
                    "https://www.khan.co.kr/rss/rssdata/newsletter_news.xml",   # ê²½í–¥ì‹ ë¬¸ / ë‰´ìŠ¤ë ˆí„°
                    "https://www.khan.co.kr/rss/rssdata/interactive_news.xml",  # ê²½í–¥ì‹ ë¬¸ / ì¸í„°ë™í‹°ë¸Œ
                    "https://www.kmib.co.kr/rss/data/kmibRssAll.xml",           # êµ­ë¯¼ì¼ë³´ / ì „ì²´ê¸°ì‚¬
                    "https://www.kmib.co.kr/rss/data/kmibPolRss.xml",           # êµ­ë¯¼ì¼ë³´ / ì •ì¹˜
                    "https://www.kmib.co.kr/rss/data/kmibEcoRss.xml",           # êµ­ë¯¼ì¼ë³´ / ê²½ì œ
                    "https://www.kmib.co.kr/rss/data/kmibSocRss.xml",           # êµ­ë¯¼ì¼ë³´ / ì‚¬íšŒ
                    "https://www.kmib.co.kr/rss/data/kmibIntRss.xml",           # êµ­ë¯¼ì¼ë³´ / êµ­ì œ
                    "https://www.kmib.co.kr/rss/data/kmibEntRss.xml",           # êµ­ë¯¼ì¼ë³´ / ì—°ì˜ˆ
                    "https://www.kmib.co.kr/rss/data/kmibSpoRss.xml",           # êµ­ë¯¼ì¼ë³´ / ìŠ¤í¬ì¸ 
                    "https://www.kmib.co.kr/rss/data/kmibGolfRss.xml",          # êµ­ë¯¼ì¼ë³´ / ê³¨í”„
                    "https://www.kmib.co.kr/rss/data/kmibLifeRss.xml",          # êµ­ë¯¼ì¼ë³´ / ë¼ì´í”„
                    "https://www.kmib.co.kr/rss/data/kmibTraRss.xml",           # êµ­ë¯¼ì¼ë³´ / ì—¬í–‰
                    "https://www.kmib.co.kr/rss/data/kmibEsportsRss.xml",       # êµ­ë¯¼ì¼ë³´ / eìŠ¤í¬ì¸ 
                    "https://www.kmib.co.kr/rss/data/kmibColRss.xml",           # êµ­ë¯¼ì¼ë³´ / ì‚¬ì„¤/ì¹¼ëŸ¼
                    "https://www.kmib.co.kr/rss/data/kmibChrRss.xml",           # êµ­ë¯¼ì¼ë³´ / ë”ë¯¸ì…˜
                    ]

# âœ… Kafka í”„ë¡œë“€ì„œ ì „ì—­ ì„¤ì • (JSON ì§ë ¬í™”)
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
)

def kafka_producer(url, full_text):
    try:
        # Kafkaë¡œ ë³´ë‚¼ ë©”ì‹œì§€ êµ¬ì„±
        message = {
            "url": url,
            "full_text": full_text
        }

        # fulltext í† í”½ìœ¼ë¡œ ì „ì†¡
        producer.send("fulltext-topic", value=message)
        print(f"ğŸ“¤ Kafka ì „ì†¡ ì™„ë£Œ: {url}")
    except Exception as e:
        print(f"âŒ Kafka ì „ì†¡ ì‹¤íŒ¨: {e}")

# âœ… ë©”ì¸ ë£¨í”„
def main_loop(interval=300):
    print(f"ğŸ” [5ë¶„ ê°„ê²©] RSS ê°ì‹œ ì‹œì‘...")

    while True:
        try:
            with engine.begin() as conn:
                for feed_url in RSS_FEED_URL_LIST:
                    print(f"\nğŸ“¡ í”¼ë“œ ì²´í¬: {feed_url}")
                    response = requests.get(feed_url, headers=headers)
                    feed = feedparser.parse(response.content)

                    duplicate_count = 0  # ì—°ì† ì¤‘ë³µ ìˆ˜
                    for entry in feed.entries:
                        url = entry.link

                        if article_exists(conn, url):
                            duplicate_count += 1
                            print(f"âš ï¸ ì¤‘ë³µ({duplicate_count}): {url}")
                            if duplicate_count >= 3:
                                print("ğŸš« ì—°ì† 3íšŒ ì¤‘ë³µ â†’ í”¼ë“œ ìŠ¤í‚µ")
                                break
                            continue
                        else:
                            duplicate_count = 0

                        title = entry.title
                        summary = entry.summary
                        write_date = entry.updated_parsed or datetime.now()
                        write_date = datetime(*write_date[:6])
                        writer = entry.author if hasattr(entry, 'author') else "Unknown"

                        full_text = get_korean_text_from_url(url)

                        if len(full_text.strip()) >= 100:
                            if writer.strip() and summary.strip():
                                insert_article(title, url, writer, summary, write_date, full_text)
                                kafka_producer(url, full_text)
                            else:
                                print(f"âš ï¸ í•„ìˆ˜ ì •ë³´ ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µ: {title}")
                                print(f"    â›” writer: {repr(writer)}, summary ê¸¸ì´: {len(summary.strip())}")
                        else:
                            print(f"âš ï¸ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ìŠ¤í‚µ: {title}")
                            print(full_text)


            print(f"\nâ± {interval}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œì‘...")
            time.sleep(interval)        

        except KeyboardInterrupt:
            print("\nğŸ›‘ ìˆ˜ì§‘ ì¤‘ë‹¨ë¨.")
            break
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
            time.sleep(interval)

# âœ… ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    main_loop(interval=300)